{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import preprocessing as prep\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from skimage.feature import corner_peaks,corner_harris,BRIEF,ORB\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import gaussian\n",
    "from sklearn.utils import shuffle\n",
    "import pdb\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Overall  dataset consists of 400 images (100 per class among four classes). \n",
    "# We are using 320 images (80 per class) as Train set+oracle set and 80 images (20 per class) as Test set\n",
    "\n",
    "# Control Parameters\n",
    "\n",
    "Method_type = 3                                 # 0:Baseline accuracy computation (Entire dataset) \n",
    "                                                # 1: Random sampling  2:Max Entropy measure  3:Our method\n",
    "Initial_dataset_size   = 104                      # Initial dataset size to sample using ORB descriptors\n",
    "No_of_images_to_sample = 10                      # No of images to sample each AL iteration, common for all method types\n",
    "No_of_images_entropy_method3 = 30               # No of images to sample initially from entropy measure (before using learnt feature vectors) for our method  \n",
    "Distance_metric = 'euclidean'                   # Distance metric to compare model feature vectors\n",
    "Create_initial_dataset = False                   # True:  Performs ORB descriptor technique (unless Load_descriptors = True) to create initial training and oracle sets from overall available dataset\n",
    "                                                #        Use this to start the active learning process from stratch for a new method_type     \n",
    "                                                # False: Loads most recent training and oracle sets stored in same directory\n",
    "                                                #        and starts from the last Active learning iteration performed for a particular method_type  \n",
    "Accuracy_array_name = 'ARSN_baseline/acc_MESS.npy'     # Numpy array storage name. This records the best test accuracy acheived during each AL iteration.\n",
    "                                                # Change name for different method_type\n",
    "AUC_array_name   = 'ARSN_baseline/AUC_MESS.npy'              # Stores the AUC score on test set for each AL iteration.Change name for different method_type\n",
    "Load_descriptors = True                        # Loads precomputed descriptors for all dataset images.It is useful only when Create_initial_dataset==True  \n",
    "\n",
    "# Network Training parameters\n",
    "Epochs = 5                                # no of epochs to train model for each AL iteration\n",
    "AL_iterations = 34                        # no of times to implement Active learning algorithm   \n",
    "Batch_size = 32\n",
    "train_from_stratch = True                 # Trains model from stratch for each AL iteration\n",
    "learning_rate = 0.0001                    # Learning rate for deep CNN model\n",
    "Img_resol     = 512                       # Input image resolution (Height == Width assumed) \n",
    "Availble_training_data_size = 444\n",
    "Availble_test_data_size     = 85+85\n",
    "No_classes = 2\n",
    "\n",
    "path       = 'model/MESSIDOR_baseline.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get images\n",
    "orc_ARSN        = np.load('dataset/ARSN_oracle.npy', encoding='bytes')\n",
    "orc_ARSN_label  = np.load('dataset/ARSN_oracle_label.npy', encoding='bytes')\n",
    "test_ARSN       = np.load('dataset/ARSN_test.npy', encoding='bytes')\n",
    "test_ARSN_label = np.load('dataset/ARSN_test_label.npy', encoding='bytes')\n",
    "\n",
    "train_MESS       = np.load('dataset/MESSIDOR_train.npy', encoding='bytes')\n",
    "train_MESS_label = np.load('dataset/MESSIDOR_train_label.npy', encoding='bytes')\n",
    "test_MESS        = np.load('dataset/MESSIDOR_test.npy', encoding='bytes')\n",
    "test_MESS_label  = np.load('dataset/MESSIDOR_test_label.npy', encoding='bytes')\n",
    "\n",
    "#Preprocess images\n",
    "orc_ARSN_prep    = np.transpose(orc_ARSN,  (0,3,1,2))\n",
    "test_ARSN_prep   = np.transpose(test_ARSN, (0,3,1,2))\n",
    "train_MESS_prep  = np.transpose(train_MESS,(0,3,1,2))\n",
    "test_MESS_prep   = np.transpose(test_MESS, (0,3,1,2))\n",
    "orc_ARSN_prep, test_ARSN_prep   = prep.clean(orc_ARSN_prep,test_ARSN_prep,Img_resol)\n",
    "train_MESS_prep, test_MESS_prep = prep.clean(train_MESS_prep,test_MESS_prep,Img_resol)\n",
    "\n",
    "#Weird\n",
    "#orc_ARSN_prep, test_MESS_prep = prep.clean(orc_ARSN,test_MESS,Img_sol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def add_file_to_train(new_index):\n",
    "    global X_train,Y_train,X_oracle,Y_oracle\n",
    "\n",
    "    X_train_new = X_oracle[new_index]\n",
    "    Y_train_new = Y_oracle[new_index]\n",
    "\n",
    "    X_train = np.concatenate((X_train,X_train_new),axis=0)\n",
    "    Y_train = np.concatenate((Y_train,Y_train_new),axis=0)\n",
    "\n",
    "    X_oracle = np.delete(X_oracle,new_index,axis=0)\n",
    "    Y_oracle = np.delete(Y_oracle,new_index,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_descriptors(X):\n",
    "    descriptor_extractor = ORB(n_keypoints=500)\n",
    "    Descriptors = []\n",
    "    for i in range(len(X)):\n",
    "        print ('Calculating ORB descriptor for image:{}'.format(i))\n",
    "        Im  = np.asarray(X[i][:,:,:],dtype='float64')\n",
    "        Max = np.amax(Im)\n",
    "        Im = Im/Max\n",
    "        Im = rgb2gray(Im)\n",
    "\n",
    "        descriptor_extractor.detect_and_extract(Im)\n",
    "        Temp = descriptor_extractor.descriptors\n",
    "        Descriptors.append(np.asarray(np.round(np.average(Temp,axis=0)),dtype='int32'))\n",
    "\n",
    "    Descriptors_matrix = np.zeros([len(X),256])\n",
    "    for i in range(len(X)):\n",
    "        Descriptors_matrix[i,:] = Descriptors[i] \n",
    "\n",
    "    return Descriptors_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(X_oracle_descriptor,X_train_descriptor):\n",
    "    Distances = np.zeros([len(X_oracle_descriptor),len(X_train_descriptor)])\n",
    "    for i in range(len(X_oracle_descriptor)):\n",
    "        Oracle = np.reshape(X_oracle_descriptor[i,:],(1,256))\n",
    "        for j in range(len(X_train_descriptor)):\n",
    "            Train = np.reshape(X_train_descriptor[j,:],(1,256))\n",
    "            Distances[i,j] = cdist(Train,Oracle,'hamming')\n",
    "\n",
    "    Distances = np.reshape(np.average(Distances,axis=1),(len(X_oracle_descriptor),))\n",
    "    Sorted_distances = np.flip(np.sort(Distances,axis=0),axis=0)\n",
    "    Sorted_indexes = np.flip(np.argsort(Distances,axis=0),axis=0)\n",
    "    return Sorted_distances,Sorted_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainset():\n",
    "    global Initial_dataset_size,X_oracle,X_oracle_descriptor,Y_oracle,Img_resol,No_classes\n",
    "    Index = []\n",
    "    Initial_sample = np.random.randint(0,len(X_oracle),1)\n",
    "    Index.append(Initial_sample[0])\n",
    "\n",
    "    X_train_descriptor =  np.reshape(X_oracle_descriptor[Initial_sample,:],(1,256))\n",
    "    X_train = np.reshape(X_oracle[Initial_sample,:,:,:],(1,Img_resol,Img_resol,3))\n",
    "    if No_classes==2:\n",
    "        Y_train = np.reshape(Y_oracle[Initial_sample],(1,))\n",
    "    else:\n",
    "        Y_train = np.reshape(Y_oracle[Initial_sample,:],(1,No_classes))\t\t\t\n",
    "\n",
    "    X_oracle = np.delete(X_oracle,Initial_sample,axis=0)\n",
    "    X_oracle_descriptor = np.delete(X_oracle_descriptor,Initial_sample,axis=0)\n",
    "    Y_oracle = np.delete(Y_oracle,Initial_sample,axis=0)\n",
    "\n",
    "    print ('Creating training dataset:')\n",
    "    for i in range(Initial_dataset_size-1):\n",
    "        Dist,Indexes = calculate_distance(X_oracle_descriptor, X_train_descriptor)\n",
    "\n",
    "        Selected_Index = Indexes[0]\n",
    "        Index.append(Selected_Index)\n",
    "\n",
    "        Addition = np.reshape(X_oracle[Selected_Index,:,:,:],(1,Img_resol,Img_resol,3))\n",
    "        X_train = np.concatenate((X_train,Addition),axis=0)\n",
    "\n",
    "        Addition = np.reshape(X_oracle_descriptor[Selected_Index,:],(1,256))\n",
    "        X_train_descriptor = np.concatenate((X_train_descriptor,Addition),axis=0)\n",
    "\n",
    "        X_oracle = np.delete(X_oracle,Selected_Index,axis=0)\n",
    "        X_oracle_descriptor = np.delete(X_oracle_descriptor,Selected_Index,axis=0)\n",
    "\n",
    "        if No_classes==2:\n",
    "            Addition = np.reshape(Y_oracle[Selected_Index],(1,))\n",
    "        else:\n",
    "            Addition = np.reshape(Y_oracle[Selected_Index],(1,No_classes))\n",
    "\n",
    "        Y_train = np.concatenate((Y_train,Addition),axis=0)\n",
    "        Y_oracle = np.delete(Y_oracle,Selected_Index,axis=0)\n",
    "\n",
    "        X_train,Y_train = shuffle(X_train,Y_train,random_state=62)\n",
    "\n",
    "    return X_train,Y_train,Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_feature_vectors(oracle_feature_vectors,train_feature_vectors,Indices):\n",
    "    global No_of_images_to_sample,Distance_metric\n",
    "    indexes = []\n",
    "    for i in range(No_of_images_to_sample):\n",
    "        compared_distances = np.reshape(np.average(cdist(oracle_feature_vectors,train_feature_vectors,Distance_metric),axis=1),(len(oracle_feature_vectors),))  \n",
    "        selected_distance_index = Indices[np.argmax(compared_distances)]\n",
    "\n",
    "        Addition = np.reshape(oracle_feature_vectors[np.argmax(compared_distances),:],(1,oracle_feature_vectors.shape[1]))\n",
    "        train_feature_vectors = np.concatenate((train_feature_vectors,Addition),axis=0)\n",
    "        oracle_feature_vectors = np.delete(oracle_feature_vectors,np.argmax(compared_distances),axis=0)\n",
    "        Indices = np.delete(Indices,np.argmax(compared_distances),axis=0)\n",
    "        for j,r in enumerate(Indices):\n",
    "            if r>selected_distance_index:\n",
    "                    Indices[j] = r-1\n",
    "        #print(selected_distance_index)\n",
    "        indexes.append(selected_distance_index)\n",
    "    add_file_to_train(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample():\n",
    "    global X_train,X_oracle,Y_train,Y_oracle,No_of_images_to_sample,Img_resol,No_classes\n",
    "\n",
    "    Indice = np.arange(0,len(X_oracle),1)\n",
    "\n",
    "    Positive_index = []\n",
    "    for i in range(No_of_images_to_sample):\n",
    "        I = np.random.randint(0,len(Indice),1)\n",
    "        Positive_index.append(Indice[I])\n",
    "        Indice = np.delete(Indice,I)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        Addition = np.reshape(np.take(X_oracle,Positive_index,axis=0),(No_of_images_to_sample,3,Img_resol,Img_resol))\n",
    "    else:\n",
    "        Addition = np.reshape(np.take(X_oracle,Positive_index,axis=0),(No_of_images_to_sample,Img_resol,Img_resol,3)) \n",
    "    X_train = np.concatenate((X_train,Addition),axis=0)\n",
    "    X_oracle = np.delete(X_oracle,Positive_index,axis=0)\n",
    "\n",
    "    if No_classes==2:\n",
    "        Addition = np.squeeze(np.take(Y_oracle,Positive_index,axis=0),axis=1)\n",
    "    else:\n",
    "        Addition = np.take(Y_oracle,Positive_index,axis=0)\n",
    "\n",
    "    Y_train = np.concatenate((Y_train,Addition),axis=0)\n",
    "    Y_oracle = np.delete(Y_oracle,Positive_index,axis=0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(oracle_preds):\n",
    "    global No_classes\n",
    "\n",
    "    Entropy_measure = np.zeros([oracle_preds.shape[0],])\n",
    "    if No_classes==2:\n",
    "        for i,r in enumerate(oracle_preds):\n",
    "            Entropy_measure[i] = -r*np.log2(r) + (1-r)*np.log2(1-r)\n",
    "    else:\n",
    "        for i in range(len(oracle_preds)):\n",
    "            for j in oracle_preds[i,:]:\n",
    "                if j!=0.0 and j!=1.0:\n",
    "                    Entropy_measure[i] += -j*np.log2(j)\n",
    "\n",
    "    return Entropy_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedalTrainer:\n",
    "    def __init__(self, model, index, batch_size, max_epochs=1, path='model.pt',\n",
    "                 method=0,sample=0,sample_cross_entropy=0):\n",
    "        global orc_ARSN_prep, orc_ARSN_label, test_ARSN_prep, test_ARSN_label, test_MESS_prep, test_MESS_label\n",
    "\n",
    "        self.model      = model\n",
    "        self.batch_size = batch_size\n",
    "        self.train_ARSN      = orc_ARSN_prep[index]\n",
    "        self.train_ARSN_label= orc_ARSN_label[index]\n",
    "        orc_index = list(set(np.arange(orc_ARSN.shape[0]).tolist()) - set(index))\n",
    "        self.orc_ARSN         = orc_ARSN_prep[orc_index]\n",
    "        self.orc_ARSN_label   = orc_ARSN_label[orc_index]     \n",
    "        self.test_ARSN        = test_ARSN_prep\n",
    "        self.test_ARSN_label  = test_ARSN_label \n",
    "        self.test_MESS        = test_MESS_prep\n",
    "        self.test_MESS_label  = test_MESS_label\n",
    "        self.train_losses = []\n",
    "        self.test_losses  = []\n",
    "        self.test_acc     = []\n",
    "        self.test_auc_score = []\n",
    "        self.method = method\n",
    "        self.sample = sample\n",
    "        self.sample_cross_entropy = sample_cross_entropy\n",
    "        self.epochs = 0\n",
    "        self.prev_epoch = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.path = path\n",
    "        \n",
    "        self.orc_ind_dict = {}\n",
    "        for i in range(len(orc_index)):\n",
    "            self.orc_ind_dict[str(i)] = orc_index[i]\n",
    "        \n",
    "        #model helper\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(),lr=0.0001, weight_decay=1e-6)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        batch_num = 0\n",
    "        index = np.random.choice(self.train_ARSN.shape[0], size=(self.train_ARSN.shape[0],), replace=False)\n",
    "        for i in range(0, self.train_ARSN.shape[0]-self.batch_size, self.batch_size):\n",
    "            inputs  = self.train_ARSN[index[i:i+self.batch_size]]\n",
    "            targets = self.train_ARSN_label[index[i:i+self.batch_size]]\n",
    "            loss = self.train_batch(inputs, targets)\n",
    "            epoch_loss += loss\n",
    "            batch_num += 1\n",
    "            #if batch_num % 10 == 0:\n",
    "                #print(\"At batch\",batch_num)\n",
    "                #print(\"Loss:\",loss)\n",
    "        epoch_loss = epoch_loss / batch_num\n",
    "        self.epochs += 1\n",
    "        if self.epochs == self.max_epochs:\n",
    "            print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f' % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        \n",
    "        return epoch_loss\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        inputs  = torch.from_numpy(inputs).to(DEVICE).float()\n",
    "        targets = torch.from_numpy(targets).to(DEVICE).float()\n",
    "        outputs = torch.flatten(self.model(inputs))\n",
    "        loss    = self.criterion(outputs,targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        auc_score = 0\n",
    "        test_acc  = 0\n",
    "        batch_num = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, self.test_ARSN.shape[0], self.batch_size):\n",
    "                inputs  = self.test_ARSN[i:i+self.batch_size]\n",
    "                targets = self.test_ARSN_label[i:i+self.batch_size]\n",
    "                loss, acc, r_sco = self.test_batch(inputs,targets)\n",
    "                test_loss += loss\n",
    "                test_acc  += acc\n",
    "                auc_score += r_sco\n",
    "                batch_num += 1\n",
    "            test_loss /= batch_num\n",
    "            auc_score  = 100* auc_score/batch_num\n",
    "            test_acc   = 100* test_acc/(batch_num*self.batch_size)\n",
    "            \n",
    "            self.test_losses.append(test_loss)\n",
    "            self.test_acc.append(test_acc)\n",
    "            self.test_auc_score.append(auc_score)\n",
    "            print('[Test]  Epoch [%d/%d]   Loss: %.4f Accuracy: %.4f AUC_Score: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, test_loss, test_acc, auc_score))\n",
    "        return test_loss, test_acc, auc_score\n",
    "            \n",
    "    def test_batch(self, inputs, targets):\n",
    "        inputs  = torch.from_numpy(inputs).to(DEVICE).float()\n",
    "        targets = torch.from_numpy(targets).to(DEVICE).float()\n",
    "        outputs = torch.flatten(self.model(inputs))\n",
    "        loss    = self.criterion(outputs,targets) \n",
    "        predicted = torch.round(outputs.data)\n",
    "        correct   = (predicted == targets).sum()\n",
    "        auc_score = roc_auc_score(targets,outputs.data)\n",
    "        \n",
    "        return loss.item(), correct.item(), auc_score\n",
    "    \n",
    "    def fit(self):\n",
    "        best_nll = 1e30\n",
    "        best_acc = 0\n",
    "        best_auc = 0\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.train()\n",
    "            nll, acc, auc = self.test()\n",
    "            if nll < best_nll:\n",
    "                best_nll = nll\n",
    "                best_acc = acc\n",
    "                best_auc = auc\n",
    "                self.save()\n",
    "        return best_nll, best_acc, best_auc\n",
    "    \n",
    "    def get_new_index(self):\n",
    "        with torch.no_grad():\n",
    "            #Get Logits of oracle set\n",
    "            inputs = torch.from_numpy(self.orc_ARSN[:len(self.orc_ARSN)//3]).to(DEVICE).float()\n",
    "            orc_out_0 = self.model(inputs).cpu().numpy()\n",
    "            inputs    = torch.from_numpy(self.orc_ARSN[len(self.orc_ARSN)//3:2*len(self.orc_ARSN)//3]).to(DEVICE).float()\n",
    "            orc_out_1 = self.model(inputs).cpu().numpy()\n",
    "            inputs    = torch.from_numpy(self.orc_ARSN[2*len(self.orc_ARSN)//3:]).to(DEVICE).float()\n",
    "            orc_out_2 = self.model(inputs).cpu().numpy()\n",
    "            orc_out = np.concatenate((orc_out_0,orc_out_1),0)\n",
    "            orc_out = np.concatenate((orc_out,orc_out_2),0)\n",
    "            \n",
    "            #Get features of lowest entropy of oracle set\n",
    "            entropy = compute_entropy(orc_out)\n",
    "            low_entrop_ind = np.flip(np.argsort(entropy),axis=0)[:self.sample_cross_entropy]\n",
    "            inputs      = torch.from_numpy(self.orc_ARSN[low_entrop_ind]).to(DEVICE).float()\n",
    "            orc_feat      = self.model.forward(inputs,extract_feature=True).cpu().numpy()\n",
    "            \n",
    "            #Get features of train set\n",
    "            inputs        = torch.from_numpy(self.train_ARSN[:len(self.train_ARSN)//2]).to(DEVICE).float() \n",
    "            train_feat_0  = self.model.forward(inputs,extract_feature=True).cpu().numpy()\n",
    "            inputs        = torch.from_numpy(self.train_ARSN[len(self.train_ARSN)//2:]).to(DEVICE).float() \n",
    "            train_feat_1  = self.model.forward(inputs,extract_feature=True).cpu().numpy()\n",
    "            train_feat    = np.concatenate((train_feat_0,train_feat_1), 0)\n",
    "            \n",
    "            compare_feature_vectors(orc_feat,train_feat,low_entrop_ind)\n",
    "            \n",
    "            new_ind = [self.orc_ind_dict[str(i)] for i in low_entrop_ind]\n",
    "            #pdb.set_trace()\n",
    "            \n",
    "            return new_ind        \n",
    "    \n",
    "    def load(self,path='model/MESSIDOR_baseline.pt'):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.prev_epoch = checkpoint['epoch']\n",
    "        self.model.load_state_dict(checkpoint['state_dict'])\n",
    "        self.model.cuda()\n",
    "        print(\"Model Loaded!\")\n",
    "        \n",
    "    def save(self,path='model/ARSN_active_learning_check.pt'):\n",
    "        self.model.cpu()\n",
    "        state = {'epoch': self.epochs+self.prev_epoch, \n",
    "                 'state_dict': self.model.state_dict()}\n",
    "        torch.save(state, path)\n",
    "        self.model.cuda()\n",
    "        print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Medal(nn.Module):\n",
    "    def __init__(self, pretrain_inception, num_classes):\n",
    "        super(Medal, self).__init__()\n",
    "        self.inception = pretrain_inception\n",
    "        self.linear_0        = nn.Linear(1000, 512)\n",
    "        self.dropout         = nn.Dropout(0.5)\n",
    "        self.linear_1        = nn.Linear(512, 128)\n",
    "        self.linear_2        = nn.Linear(128, num_classes)\n",
    "        self.relu            = nn.ReLU()\n",
    "        self.sigmoid         = nn.Sigmoid()\n",
    "        \n",
    "        self.init_weight()\n",
    "        \n",
    "    def forward(self, images,extract_feature=False):\n",
    "        if extract_feature:\n",
    "            out = self.inception(images,extract_mixed_6=True)\n",
    "            out = nn.AdaptiveAvgPool2d((1,1))(out)\n",
    "            return torch.squeeze(out)\n",
    "        \n",
    "        out = self.inception(images)\n",
    "        out = self.linear_0(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.linear_1(out))\n",
    "        out = self.linear_2(out)\n",
    "        \n",
    "        return self.sigmoid(out)\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.linear_0.weight.data.uniform_(-0.1,0.1)\n",
    "        self.linear_1.weight.data.uniform_(-0.1,0.1)\n",
    "        self.linear_2.weight.data.uniform_(-0.1,0.1)\n",
    "        \n",
    "        self.linear_0.bias.data.fill_(0)\n",
    "        self.linear_1.bias.data.fill_(0)\n",
    "        self.linear_2.bias.data.fill_(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inception model\n",
    "inception = models.inception_v3()\n",
    "\n",
    "# Inception v3 pretrained weights\n",
    "model_dir = '/afs/ece.cmu.edu/usr/sadom/Private/Active_Learning/pretrained_model'\n",
    "model_urls = {'inception_v3_google': \n",
    "    'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',}\n",
    "inception.load_state_dict(model_zoo.load_url(\n",
    "    model_urls['inception_v3_google'],model_dir=model_dir))\n",
    "inception.aux_logits = False\n",
    "\n",
    "model   = Medal(inception,1)\n",
    "model   = nn.DataParallel(model)\n",
    "model   = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length:(264, 512, 512, 3)\n",
      "############################################################\n",
      "Performing Active learning iteration 19 for method 3\n",
      "Set sizes used for current AL iteration:\n",
      "Training set size:264\n",
      "Oracle set size:187\n",
      "Model Loaded!\n",
      "[Test]  Epoch [1/5]   Loss: 0.7895 Accuracy: 75.0000 AUC_Score: 77.1219\n",
      "Model saved!\n",
      "[Test]  Epoch [2/5]   Loss: 0.6443 Accuracy: 76.5625 AUC_Score: 71.2539\n",
      "Model saved!\n",
      "[Test]  Epoch [3/5]   Loss: 0.8741 Accuracy: 76.5625 AUC_Score: 70.6462\n",
      "[Test]  Epoch [4/5]   Loss: 1.0224 Accuracy: 75.0000 AUC_Score: 71.3735\n",
      "[TRAIN]  Epoch [5/5]   Loss: 0.0298\n",
      "[Test]  Epoch [5/5]   Loss: 1.3626 Accuracy: 75.5208 AUC_Score: 69.2612\n",
      "Positive Class count:236\n",
      "Negative Class count:28\n",
      "Best Test accuracy achieved for this AL iteration:76.5625 AUC score:71.25385802469137\n",
      "############################################################\n",
      "Performing Active learning iteration 20 for method 3\n",
      "Set sizes used for current AL iteration:\n",
      "Training set size:274\n",
      "Oracle set size:177\n",
      "Model Loaded!\n",
      "[Test]  Epoch [1/5]   Loss: 0.5522 Accuracy: 76.5625 AUC_Score: 77.9668\n",
      "Model saved!\n",
      "[Test]  Epoch [2/5]   Loss: 0.5980 Accuracy: 75.5208 AUC_Score: 75.1427\n",
      "[Test]  Epoch [3/5]   Loss: 0.8703 Accuracy: 77.0833 AUC_Score: 73.7172\n",
      "[Test]  Epoch [4/5]   Loss: 1.0482 Accuracy: 76.0417 AUC_Score: 73.6535\n",
      "[TRAIN]  Epoch [5/5]   Loss: 0.0084\n",
      "[Test]  Epoch [5/5]   Loss: 1.2190 Accuracy: 75.5208 AUC_Score: 73.3864\n",
      "Positive Class count:245\n",
      "Negative Class count:29\n",
      "Best Test accuracy achieved for this AL iteration:76.5625 AUC score:77.96682098765433\n",
      "############################################################\n",
      "Performing Active learning iteration 21 for method 3\n",
      "Set sizes used for current AL iteration:\n",
      "Training set size:284\n",
      "Oracle set size:167\n",
      "Model Loaded!\n",
      "[Test]  Epoch [1/5]   Loss: 0.6339 Accuracy: 75.0000 AUC_Score: 76.3349\n",
      "Model saved!\n",
      "[Test]  Epoch [2/5]   Loss: 0.9051 Accuracy: 76.5625 AUC_Score: 72.8627\n",
      "[Test]  Epoch [3/5]   Loss: 1.1750 Accuracy: 75.5208 AUC_Score: 71.2664\n",
      "[Test]  Epoch [4/5]   Loss: 1.4252 Accuracy: 76.5625 AUC_Score: 66.5403\n",
      "[TRAIN]  Epoch [5/5]   Loss: 0.0105\n",
      "[Test]  Epoch [5/5]   Loss: 1.6819 Accuracy: 75.0000 AUC_Score: 65.5392\n",
      "Positive Class count:252\n",
      "Negative Class count:32\n",
      "Best Test accuracy achieved for this AL iteration:75.0 AUC score:76.33487654320987\n",
      "############################################################\n",
      "Performing Active learning iteration 22 for method 3\n",
      "Set sizes used for current AL iteration:\n",
      "Training set size:294\n",
      "Oracle set size:158\n",
      "Model Loaded!\n",
      "[Test]  Epoch [1/5]   Loss: 0.4825 Accuracy: 77.0833 AUC_Score: 80.2971\n",
      "Model saved!\n",
      "[Test]  Epoch [2/5]   Loss: 0.6892 Accuracy: 76.5625 AUC_Score: 72.6254\n",
      "[Test]  Epoch [3/5]   Loss: 0.8350 Accuracy: 75.5208 AUC_Score: 71.9309\n",
      "[Test]  Epoch [4/5]   Loss: 1.3870 Accuracy: 77.0833 AUC_Score: 73.7867\n",
      "[TRAIN]  Epoch [5/5]   Loss: 0.0052\n",
      "[Test]  Epoch [5/5]   Loss: 1.6602 Accuracy: 77.0833 AUC_Score: 72.1807\n",
      "Positive Class count:261\n",
      "Negative Class count:33\n",
      "Best Test accuracy achieved for this AL iteration:77.08333333333333 AUC score:80.29706790123458\n"
     ]
    }
   ],
   "source": [
    "X_oracle = orc_ARSN\n",
    "Y_oracle = orc_ARSN_label\n",
    "X_test   = test_MESS\n",
    "Y_test   = test_MESS_label\n",
    "\n",
    "if Method_type==0:\n",
    "    print ('Training size: {}'.format(X_train.shape))\n",
    "    print ('###########################################################')\n",
    "    print ('Evaluating baseline performance...')\n",
    "    Index = np.arange(len(X_oracle)).tolist()\n",
    "    Epochs = 50\n",
    "    trainer = MedalTrainer(model,Index,Batch_size,Epochs,path)\n",
    "    loss, accuracy, auc_score = trainer.fit()\n",
    "    print ('Baseline accuracy calculated:{}'.format(accuracy)) \n",
    "else:\n",
    "    if Create_initial_dataset==True:\n",
    "        if Load_descriptors==False: \n",
    "            X_oracle_descriptor = calculate_descriptors(X_oracle)\n",
    "            np.save('dataset/ARSN_descriptor.npy',X_oracle_descriptor)\n",
    "            print ('Descriptor array saved!!')\n",
    "        else:\n",
    "            X_oracle_descriptor = np.load('dataset/ARSN_descriptor.npy',encoding='bytes')\n",
    "\n",
    "        X_train,Y_train,Index = create_trainset()\n",
    "\n",
    "        np.save('ARSN_baseline/X_train.npy',X_train)\n",
    "        np.save('ARSN_baseline/Y_train.npy',Y_train)\n",
    "        np.save('ARSN_baseline/Index.npy', Index)\n",
    "        np.save('ARSN_baseline/X_oracle.npy',X_oracle)\n",
    "        np.save('ARSN_baseline/Y_oracle.npy',Y_oracle)\n",
    "        \n",
    "        Test_Accuracy = []\n",
    "        AUC_score = []\n",
    "        Class_count = []\n",
    "        current_AL_iteration = 0\n",
    "\n",
    "    else:\n",
    "        X_train  = np.load('ARSN_baseline/X_train.npy', encoding='bytes')\n",
    "        Y_train  = np.load('ARSN_baseline/Y_train.npy', encoding='bytes')\n",
    "        X_oracle = np.load('ARSN_baseline/X_oracle.npy',encoding='bytes')\n",
    "        Y_oracle = np.load('ARSN_baseline/Y_oracle.npy',encoding='bytes')\n",
    "        Index    = np.load('ARSN_baseline/Index.npy',   encoding='bytes').tolist()\n",
    "\n",
    "        Test_Accuracy = np.load(Accuracy_array_name,encoding='bytes').tolist()\n",
    "        AUC_score = np.load(AUC_array_name,encoding='bytes').tolist()\n",
    "        current_AL_iteration = np.load('ARSN_baseline/Current_AL_iteration.npy',encoding='bytes')[0]\n",
    "        Class_count = list(np.load('ARSN_baseline/Class_count.npy',encoding='bytes'))\n",
    "\n",
    "    best_auc = 86.46    \n",
    "    print ('Train length:{}'.format(X_train.shape))\n",
    "    for i in range(current_AL_iteration+1,AL_iterations+1):\n",
    "        print ('############################################################')\n",
    "        print ('Performing Active learning iteration {} for method {}'.format(i,Method_type))\n",
    "        print ('Set sizes used for current AL iteration:')\n",
    "        print ('Training set size:{}'.format(len(X_train)))\n",
    "        print ('Oracle set size:{}'  .format(len(X_oracle)))\n",
    "\n",
    "        trainer = MedalTrainer(model,Index,Batch_size,Epochs,path,Method_type,\n",
    "                               No_of_images_to_sample,No_of_images_entropy_method3)\n",
    "        trainer.load()\n",
    "        loss, accuracy, auc_score = trainer.fit()\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            trainer.save(path='model/ARSN_active_learning.pt')\n",
    "        \n",
    "        AUC_score.append(auc_score)        \n",
    "        Test_Accuracy.append(accuracy)\n",
    "        np.save(Accuracy_array_name,np.array(Test_Accuracy))        \n",
    "        np.save(AUC_array_name,np.array(AUC_score))\n",
    "        np.save('ARSN_baseline/Current_AL_iteration.npy',np.array([i]))\n",
    "        np.save('ARSN_baseline/Class_count.npy',np.asarray(Class_count))\n",
    "        \n",
    "        Positive_count = list(Y_train).count(1)\n",
    "        Negative_count = list(Y_train).count(0)     \n",
    "        Class_count.append([Positive_count,Negative_count])\n",
    "        print ('Positive Class count:{}'.format(Positive_count))\n",
    "        print ('Negative Class count:{}'.format(Negative_count))\n",
    "        print ('Best Test accuracy achieved for this AL iteration:{0} AUC score:{1}'.format(accuracy,auc_score))\n",
    "\n",
    "        new_ind = trainer.get_new_index()\n",
    "        for ind in new_ind:\n",
    "            Index.append(ind)\n",
    "        \n",
    "        np.save('ARSN_baseline/Index.npy', Index)\n",
    "        np.save('ARSN_baseline/X_train.npy',X_train)\n",
    "        np.save('ARSN_baseline/Y_train.npy',Y_train)\n",
    "        np.save('ARSN_baseline/X_oracle.npy',X_oracle)\n",
    "        np.save('ARSN_baseline/Y_oracle.npy',Y_oracle)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
